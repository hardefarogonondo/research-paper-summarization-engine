{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20203c0f",
   "metadata": {},
   "source": [
    "# I. Project Team Members"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aa3d7f4",
   "metadata": {},
   "source": [
    "| Prepared by | Email | Prepared for |\n",
    "| :-: | :-: | :-: |\n",
    "| **Hardefa Rogonondo** | hardefarogonondo@gmail.com | **Research Paper Summarization Engine** |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b05cd469",
   "metadata": {},
   "source": [
    "# II. Notebook Target Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47bae1d2",
   "metadata": {},
   "source": [
    "_Insert Text Here_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3213f42d",
   "metadata": {},
   "source": [
    "# III. Notebook Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb5c3810",
   "metadata": {},
   "source": [
    "## III.A. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac84c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup, PegasusForConditionalGeneration, PegasusTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017afc00-c0f2-4e73-9306-27fa00d492f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device name: NVIDIA GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "if cuda_available:\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7291e85b",
   "metadata": {},
   "source": [
    "## III.B. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f425995",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('../../data/processed/train_df_processed.pkl')\n",
    "test_df= pd.read_pickle('../../data/processed/test_df_processed.pkl')\n",
    "validation_df = pd.read_pickle('../../data/processed/validation_df_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d849198a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Due to the success of deep learning to solving...</td>\n",
       "      <td>We provide necessary and sufficient analytical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The backpropagation (BP) algorithm is often th...</td>\n",
       "      <td>Biologically plausible learning algorithms, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We introduce the 2-simplicial Transformer, an ...</td>\n",
       "      <td>We introduce the 2-simplicial Transformer and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We present Tensor-Train RNN (TT-RNN), a novel ...</td>\n",
       "      <td>Accurate forecasting over very long time horiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Recent efforts on combining deep models with p...</td>\n",
       "      <td>We propose a variational message-passing algor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  Due to the success of deep learning to solving...   \n",
       "1  The backpropagation (BP) algorithm is often th...   \n",
       "2  We introduce the 2-simplicial Transformer, an ...   \n",
       "3  We present Tensor-Train RNN (TT-RNN), a novel ...   \n",
       "4  Recent efforts on combining deep models with p...   \n",
       "\n",
       "                                              target  \n",
       "0  We provide necessary and sufficient analytical...  \n",
       "1  Biologically plausible learning algorithms, pa...  \n",
       "2  We introduce the 2-simplicial Transformer and ...  \n",
       "3  Accurate forecasting over very long time horiz...  \n",
       "4  We propose a variational message-passing algor...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d3fa463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Incremental class learning involves sequential...</td>\n",
       "      <td>FearNet is a memory efficient neural-network, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Multi-view learning can provide self-supervisi...</td>\n",
       "      <td>Multi-view learning improves unsupervised sent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We show how discrete objects can be learnt in ...</td>\n",
       "      <td>We show how discrete objects can be learnt in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Most recent gains in visual recognition have o...</td>\n",
       "      <td>A large-scale dataset for training attention m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In recent years, deep neural networks have dem...</td>\n",
       "      <td>We proposed a time-efficient defense method ag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  Incremental class learning involves sequential...   \n",
       "1  Multi-view learning can provide self-supervisi...   \n",
       "2  We show how discrete objects can be learnt in ...   \n",
       "3  Most recent gains in visual recognition have o...   \n",
       "4  In recent years, deep neural networks have dem...   \n",
       "\n",
       "                                              target  \n",
       "0  FearNet is a memory efficient neural-network, ...  \n",
       "1  Multi-view learning improves unsupervised sent...  \n",
       "2  We show how discrete objects can be learnt in ...  \n",
       "3  A large-scale dataset for training attention m...  \n",
       "4  We proposed a time-efficient defense method ag...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1433f20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mixed precision training (MPT) is becoming a p...</td>\n",
       "      <td>We devise adaptive loss scaling to improve mix...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Many real-world problems, e.g. object detectio...</td>\n",
       "      <td>We present a novel approach for learning to pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foveation is an important part of human vision...</td>\n",
       "      <td>We compare object recognition performance on i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We explore the concept of co-design in the con...</td>\n",
       "      <td>We develop methods to train deep neural models...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Batch Normalization (BatchNorm) has shown to b...</td>\n",
       "      <td>Investigation of how BatchNorm causes adversar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  Mixed precision training (MPT) is becoming a p...   \n",
       "1  Many real-world problems, e.g. object detectio...   \n",
       "2  Foveation is an important part of human vision...   \n",
       "3  We explore the concept of co-design in the con...   \n",
       "4  Batch Normalization (BatchNorm) has shown to b...   \n",
       "\n",
       "                                              target  \n",
       "0  We devise adaptive loss scaling to improve mix...  \n",
       "1  We present a novel approach for learning to pr...  \n",
       "2  We compare object recognition performance on i...  \n",
       "3  We develop methods to train deep neural models...  \n",
       "4  Investigation of how BatchNorm causes adversar...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f59e32c9",
   "metadata": {},
   "source": [
    "# IV. Models Training and Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db0a5756",
   "metadata": {},
   "source": [
    "## IV.A. Data Shape Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aad7798b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1991, 2), (618, 2), (618, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape, validation_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3389b1bf",
   "metadata": {},
   "source": [
    "## IV.B. Data Information Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6d7319a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1991 entries, 0 to 1991\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   source  1991 non-null   object\n",
      " 1   target  1991 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 46.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe621948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 618 entries, 0 to 617\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   source  618 non-null    object\n",
      " 1   target  618 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 9.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1aef8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 618 entries, 0 to 618\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   source  618 non-null    object\n",
      " 1   target  618 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 14.5+ KB\n"
     ]
    }
   ],
   "source": [
    "validation_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce34b86f",
   "metadata": {},
   "source": [
    "## IV.C. Models Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e911796-108d-4971-bb1a-d380a5d5c28d",
   "metadata": {},
   "source": [
    "### IV.C.1. Random Seed Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d67a5248-bc1e-424e-813f-c2cc0de04327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=777):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "893bc015-9ad9-4be3-ab58-d50eec0abfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9eda58-6d11-44ae-b60b-0d6687ce1ce5",
   "metadata": {},
   "source": [
    "### IV.C.2. Tokenizer and Data Loader Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25b65c84-5a93-4854-8bf9-23bd61a6b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, tokenizer, text_list, summary_list, max_length=512):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "        for text, summary in zip(text_list, summary_list):\n",
    "            encodings = tokenizer(\n",
    "                text,\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            target_encodings = tokenizer(\n",
    "                summary,\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            self.input_ids.append(encodings.input_ids)\n",
    "            self.attn_masks.append(encodings.attention_mask)\n",
    "            self.labels.append(target_encodings.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx].flatten(),\n",
    "            \"attention_mask\": self.attn_masks[idx].flatten(),\n",
    "            \"labels\": self.labels[idx].flatten()\n",
    "        }\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "def prepare_data(tokenizer, dfs, batch_size):\n",
    "    datasets = {\n",
    "        split: SummarizationDataset(\n",
    "            tokenizer,\n",
    "            df[\"source\"].tolist(),\n",
    "            df[\"target\"].tolist()\n",
    "        ) for split, df in dfs.items()\n",
    "    }\n",
    "    loaders = {\n",
    "        f\"{split}_loader\": DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=(split == \"train\")\n",
    "        ) for split, dataset in datasets.items()\n",
    "    }\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b75e85c1-9e95-4338-914a-09cb1216fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    \"train\": train_df,\n",
    "    \"test\": test_df,\n",
    "    \"validation\": validation_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aabcec1a-b058-4ad0-b83f-8416323c30ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)\n",
    "t5_loaders = prepare_data(t5_tokenizer, dataframes, batch_size=16)\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
    "pegasus_loaders = prepare_data(pegasus_tokenizer, dataframes, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7533d-8ea0-42d2-baf6-94b0ec9633c7",
   "metadata": {},
   "source": [
    "### IV.C.3. Load Pre-Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "699ae07f-8987-4dd7-a88e-cbd167ec24e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_and_optimizer(model_name, device, learning_rate):\n",
    "    if model_name.startswith(\"t5\"):\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "    elif model_name.startswith(\"google/pegasus\"):\n",
    "        model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model. Please use 't5-small' or 'google/pegasus-xsum'.\")\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, optimizer, device, train_loader, test_loader=None, epochs=3, accumulation_steps=4):\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "    training_stats = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar = tqdm(enumerate(train_loader, start=1), total=len(train_loader), desc=f\"Epoch {epoch+1}\", leave=True)\n",
    "        for step, batch in progress_bar:\n",
    "            with autocast():\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss / accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            if step % accumulation_steps == 0 or step == len(train_loader):\n",
    "                scaler.unscale_(optimizer)\n",
    "                clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": total_loss / (step + 1)})\n",
    "        average_train_loss = total_loss / len(train_loader)\n",
    "        training_stats.append({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"Training Loss\": average_train_loss,\n",
    "        })\n",
    "        print(f\"Epoch {epoch+1} | Average Training Loss: {average_train_loss}\")\n",
    "        if test_loader:\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    test_loss += loss.item()\n",
    "            average_test_loss = test_loss / len(test_loader)\n",
    "            training_stats[-1][\"Test Loss\"] = average_test_loss\n",
    "            print(f\"Epoch {epoch+1} | Test Loss: {average_test_loss}\")\n",
    "            scheduler.step(average_test_loss)\n",
    "    plt.plot([stats[\"epoch\"] for stats in training_stats], [stats[\"Training Loss\"] for stats in training_stats], label=\"Training Loss\")\n",
    "    if \"Test Loss\" in training_stats[0]:\n",
    "        plt.plot([stats[\"epoch\"] for stats in training_stats], [stats[\"Test Loss\"] for stats in training_stats], label=\"Test Loss\")\n",
    "    plt.title(\"Training & Test Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "731444df-1dea-4620-a3b2-bc51245fefe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model_name = 't5-small'\n",
    "t5_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "t5_learning_rate = 1e-4\n",
    "t5_batch_size = 16\n",
    "t5_model, t5_optimizer = initialize_model_and_optimizer(t5_model_name, t5_device, t5_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f8827a1-d702-44d3-9e4f-21f02942fee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pegasus_model_name = 'google/pegasus-xsum'\n",
    "pegasus_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pegasus_learning_rate = 1e-5\n",
    "pegasus_batch_size = 1\n",
    "pegasus_model, pegasus_optimizer = initialize_model_and_optimizer(pegasus_model_name, pegasus_device, pegasus_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f788a82f-35de-464d-a6a0-9b7853146b3f",
   "metadata": {},
   "source": [
    "### IV.C.4. Pre-Trained Models Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15590f9d-f738-4c6a-ab8e-91cc7fc12cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_try_t5 = train_df[\"source\"].iloc[0]\n",
    "t5_input_ids_trial = t5_tokenizer.encode(\"Summarize: \" + text_to_try_t5, return_tensors='pt').to(t5_device)\n",
    "t5_summary_ids_trial = t5_model.generate(t5_input_ids_trial, max_length=200, min_length=30, length_penalty=1.5, num_beams=6, early_stopping=True)\n",
    "t5_summary_trial = t5_tokenizer.decode(t5_summary_ids_trial[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab9e866e-9b10-4f5f-b64f-2203694a8825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Due to the success of deep learning to solving a variety of challenging machine learning tasks, there is a rising interest in understanding loss functions for training neural networks from a theoretical aspect. Particularly, the properties of critical points and the landscape around them are of importance to determine the convergence performance of optimization algorithms. In this paper, we provide a necessary and sufficient characterization of the analytical forms for the critical points (as well as global minimizers) of the square loss functions for linear neural networks. We show that the analytical forms of the critical points characterize the values of the corresponding loss functions as well as the necessary and sufficient conditions to achieve global minimum. Furthermore, we exploit the analytical forms of the critical points to characterize the landscape properties for the loss functions of linear neural networks and shallow ReLU networks. One particular conclusion is that: While the loss function of linear networks has no spurious local minimum, the loss function of one-hidden-layer nonlinear networks with ReLU activation function does have local minimum that is not global minimum.\n",
      "T5 Summary: . Summarize: Due to the success of deep learning to solving a variety of challenging machine learning tasks, there is a rising interest in understanding loss functions for training neural networks from a theoretical aspect. In this paper, we provide a necessary and sufficient characterization of the analytical forms of the critical points to characterize the landscape properties for the loss functions of linear neural networks.\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text:\", text_to_try_t5)\n",
    "print(\"T5 Summary:\", t5_summary_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "819cc21e-da79-4d94-9fb1-e7558b66f994",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_try_pegasus = train_df[\"source\"].iloc[0]\n",
    "pegasus_input_ids_trial = pegasus_tokenizer.encode(text_to_try_pegasus, return_tensors='pt').to(pegasus_device)\n",
    "pegasus_summary_ids_trial = pegasus_model.generate(pegasus_input_ids_trial, max_length=200, min_length=30, length_penalty=1.5, num_beams=6, early_stopping=True)\n",
    "pegasus_summary_trial = pegasus_tokenizer.decode(pegasus_summary_ids_trial[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9be3fc1-34e4-4000-a0d1-837633ba89dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Due to the success of deep learning to solving a variety of challenging machine learning tasks, there is a rising interest in understanding loss functions for training neural networks from a theoretical aspect. Particularly, the properties of critical points and the landscape around them are of importance to determine the convergence performance of optimization algorithms. In this paper, we provide a necessary and sufficient characterization of the analytical forms for the critical points (as well as global minimizers) of the square loss functions for linear neural networks. We show that the analytical forms of the critical points characterize the values of the corresponding loss functions as well as the necessary and sufficient conditions to achieve global minimum. Furthermore, we exploit the analytical forms of the critical points to characterize the landscape properties for the loss functions of linear neural networks and shallow ReLU networks. One particular conclusion is that: While the loss function of linear networks has no spurious local minimum, the loss function of one-hidden-layer nonlinear networks with ReLU activation function does have local minimum that is not global minimum.\n",
      "T5 Summary: In this paper, we study the loss functions for training neural networks in terms of the critical points (as well as globalrs) of the square loss functions and the landscape around them.\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text:\", text_to_try_pegasus)\n",
    "print(\"T5 Summary:\", pegasus_summary_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f75fa-92f3-4634-8e21-a846114f2e82",
   "metadata": {},
   "source": [
    "### IV.C.4. T5 Small Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9009629-fb3f-40f2-9fe6-7ebc73390cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_train_stats = train_and_evaluate(\n",
    "    model=t5_model,\n",
    "    optimizer=t5_optimizer,\n",
    "    device=t5_device,\n",
    "    train_loader=t5_loaders[\"train_loader\"],\n",
    "    test_loader=t5_loaders[\"test_loader\"],\n",
    "    epochs=10,\n",
    "    accumulation_steps=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8479a288-fa00-46c9-a1d6-ef5148c8a4cd",
   "metadata": {},
   "source": [
    "### IV.C.5. PEGASUS Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c435a91d-4d8e-4461-b5e6-6c10aed0e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "pegasus_train_stats = train_and_evaluate(\n",
    "    model=pegasus_model,\n",
    "    optimizer=pegasus_optimizer,\n",
    "    device=pegasus_device,\n",
    "    train_loader=pegasus_loaders[\"train_loader\"],\n",
    "    test_loader=pegasus_loaders[\"test_loader\"],\n",
    "    epochs=3,\n",
    "    accumulation_steps=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e5f09b-6828-4eac-8b0a-757b1e13b3ef",
   "metadata": {},
   "source": [
    "## IV.D. Models Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21f14d9-88d7-4ba6-b116-929db1b76c89",
   "metadata": {},
   "source": [
    "### IV.D.1. Baseline Model Performance Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9c113-e755-4ad4-9eda-8a27463afd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summarization_model(model, model_name, tokenizer, data_loaders, device):\n",
    "    model.eval()\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    results = []\n",
    "    for key, data_loader in data_loaders.items():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=150, num_beams=1)\n",
    "            pred_summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            true_summaries = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "            rouge.add_batch(predictions=pred_summaries, references=true_summaries)\n",
    "        final_scores = rouge.compute()\n",
    "        print(final_scores)\n",
    "        result = {\n",
    "            metric: score * 100 for metric, score in final_scores.items()\n",
    "        }\n",
    "        result[\"model\"] = model_name\n",
    "        result[\"dataset\"] = key\n",
    "        results.append(result)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa4c7b-ba67-48fe-82aa-33dbc975ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders = {\n",
    "    \"train\": t5_loaders[\"train_loader\"],\n",
    "    \"validation\": t5_loaders[\"validation_loader\"],\n",
    "    \"test\": t5_loaders[\"test_loader\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2036f-cbcd-4809-8a62-dcd5b17e3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_metrics = evaluate_summarization_model(t5_model, \"T5\", t5_tokenizer, data_loaders, t5_device)\n",
    "t5_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7237d9ae-c878-454c-b382-f6cbe7fcb324",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model.eval()\n",
    "data_loader = t5_loaders[\"validation_loader\"]  # For example, to check validation dataset\n",
    "\n",
    "for i, batch in enumerate(data_loader):\n",
    "    if i > 2:  # Inspect the first few batches\n",
    "        break\n",
    "    input_ids = batch[\"input_ids\"].to(t5_device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(t5_device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = t5_model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Decode generated ids to text\n",
    "    decoded_summaries = t5_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    print(\"Generated Summaries:\", decoded_summaries)\n",
    "\n",
    "    # If using the datasets library for metric calculation\n",
    "    true_summaries = t5_tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "    print(\"True Summaries:\", true_summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ea783-8e5f-4403-a451-627395cde352",
   "metadata": {},
   "outputs": [],
   "source": [
    "pegasus_model.eval()\n",
    "data_loader = pegasus_loaders[\"validation_loader\"]  # For example, to check validation dataset\n",
    "\n",
    "for i, batch in enumerate(data_loader):\n",
    "    if i > 2:  # Inspect the first few batches\n",
    "        break\n",
    "    input_ids = batch[\"input_ids\"].to(pegasus_device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(pegasus_device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = pegasus_model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Decode generated ids to text\n",
    "    decoded_summaries = pegasus_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    print(\"Generated Summaries:\", decoded_summaries)\n",
    "\n",
    "    # If using the datasets library for metric calculation\n",
    "    true_summaries = pegasus_tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "    print(\"True Summaries:\", true_summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b672111a-4f04-4051-9f23-57b65a0f7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders = {\n",
    "    \"train\": pegasus_loaders[\"train_loader\"],\n",
    "    \"validation\": pegasus_loaders[\"validation_loader\"],\n",
    "    \"test\": pegasus_loaders[\"test_loader\"]\n",
    "}\n",
    "\n",
    "df_t5 = evaluate_summarization_model(pegasus_model, \"T5\", pegasus_tokenizer, data_loaders, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bb2fc6d",
   "metadata": {},
   "source": [
    "### IV.D.2. Export Baseline Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9f028ad-098a-4ba3-82c0-d178f8a48b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'num_beams': 8, 'length_penalty': 0.6, 'forced_eos_token_id': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../../models/original/pegasus_model\\\\tokenizer_config.json',\n",
       " '../../models/original/pegasus_model\\\\special_tokens_map.json',\n",
       " '../../models/original/pegasus_model\\\\spiece.model',\n",
       " '../../models/original/pegasus_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_save_directory = '../../models/original/t5_model'\n",
    "t5_model.save_pretrained(t5_save_directory)\n",
    "t5_tokenizer.save_pretrained(t5_save_directory)\n",
    "\n",
    "pegasus_save_directory = '../../models/original/pegasus_model'\n",
    "pegasus_model.save_pretrained(pegasus_save_directory)\n",
    "pegasus_tokenizer.save_pretrained(pegasus_save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
